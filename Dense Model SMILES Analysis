from sklearn.preprocessing import OneHotEncoder
import numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from keras.layers import Activation
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import pandas as pd
import sklearn
# demonstrate data normalization with sklearn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import explained_variance_score
from keras import optimizers

df_62k = pd.read_json('/Users/minhpham/Desktop/Machine Learning Project/m1507656/df_62k.json', orient='split')
x = df_62k['number_of_atoms']
y = df_62k['total_energy_pbe']
x = np.array(x)
y = np.array(y/1000)

'''y = y[:,np.newaxis]
x = x[:,np.newaxis]
scaler = MinMaxScaler()
scaler1 = MinMaxScaler()
# fit scaler on data
scaler.fit(y)
scaler1.fit(x)
# apply transform
y= scaler.transform(y)
x = scaler1.transform(x)'''
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.01)


# inverse transform

# inverse transform

model = Sequential()

model.add(Dense(128, input_dim=1, kernel_initializer='normal', activation='relu'))
model.add(Dense(64, activation='tanh'))
model.add(Dense(32, activation='tanh'))
model.add(Dense(16, activation='tanh'))
model.add(Dense(8, activation='tanh'))
model.add(Dense(4, activation='tanh'))
model.add(Dense(2, activation='tanh'))
model.add(Dense(1, activation='tanh'))
model.summary()
print(model.summary())

sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.0, nesterov=True)
model.compile(loss='mean_squared_error', optimizer="sgd", metrics=['mean_squared_error'])

model.fit(X_train, y_train,  epochs=8)

y_pred = model.predict(X_test)
scaler = MinMaxScaler(feature_range=(-150, 0))
scaler.fit(y_pred)

y_pred = scaler.transform(y_pred)
y_pred = np.array(y_pred)

'''y_test = scaler.inverse_transform(y_test)'''
print(y_pred[0:20])
print('-----')
print(y_test[0:20])
print(-explained_variance_score(y_test, y_pred))
